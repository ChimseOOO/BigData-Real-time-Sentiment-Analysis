{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59932,"status":"ok","timestamp":1690111930091,"user":{"displayName":"Lê Hoà","userId":"06201887992311635021"},"user_tz":-420},"id":"oE43Ydoj_pm-","outputId":"cfa2837f-35e1-42cc-c189-54e7fb7934f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting spark-nlp\n","  Downloading spark_nlp-5.0.1-py2.py3-none-any.whl (499 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.0/499.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: spark-nlp\n","Successfully installed spark-nlp-5.0.1\n","Collecting pyspark\n","  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=33f74503473684810e87e0357bd5c49ba5103d796f67ce58aaeae2412984c6c0\n","  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.1\n"]}],"source":["!pip install spark-nlp\n","!pip install pyspark"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"hq1NsqBKPHhP","executionInfo":{"status":"ok","timestamp":1690111930600,"user_tz":-420,"elapsed":518,"user":{"displayName":"Lê Hoà","userId":"06201887992311635021"}}},"outputs":[],"source":["from pyspark.sql import SparkSession, DataFrame"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"pCbL7fcT_7gD","executionInfo":{"status":"ok","timestamp":1690111990469,"user_tz":-420,"elapsed":59873,"user":{"displayName":"Lê Hoà","userId":"06201887992311635021"}}},"outputs":[],"source":["import sparknlp\n","\n","# Start Spark Session\n","spark = sparknlp.start()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hMMhh8xKO0tr","executionInfo":{"status":"ok","timestamp":1690111991973,"user_tz":-420,"elapsed":1521,"user":{"displayName":"Lê Hoà","userId":"06201887992311635021"}}},"outputs":[],"source":["spark = SparkSession.builder.appName(\"sentimentanalysis\").getOrCreate()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qRlGkZcwxlUQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690112006475,"user_tz":-420,"elapsed":14506,"user":{"displayName":"Lê Hoà","userId":"06201887992311635021"}},"outputId":"0e50076c-f8d8-43d2-f4b6-b6440c190e8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/Big Data Model/model.zip\n","   creating: content/Model/\n","   creating: content/Model/stages/\n","   creating: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/\n","   creating: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/fields/\n","   creating: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/fields/rules/\n"," extracting: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/fields/rules/.part-00000.crc  \n"," extracting: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/fields/rules/._SUCCESS.crc  \n","  inflating: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/fields/rules/part-00001  \n","  inflating: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/fields/rules/part-00000  \n"," extracting: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/fields/rules/_SUCCESS  \n"," extracting: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/fields/rules/.part-00001.crc  \n","   creating: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/metadata/\n"," extracting: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/metadata/.part-00000.crc  \n"," extracting: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/metadata/._SUCCESS.crc  \n","  inflating: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/metadata/part-00000  \n"," extracting: content/Model/stages/1_REGEX_TOKENIZER_db011187f57d/metadata/_SUCCESS  \n","   creating: content/Model/stages/0_DocumentAssembler_63357943d98f/\n","   creating: content/Model/stages/0_DocumentAssembler_63357943d98f/metadata/\n"," extracting: content/Model/stages/0_DocumentAssembler_63357943d98f/metadata/.part-00000.crc  \n"," extracting: content/Model/stages/0_DocumentAssembler_63357943d98f/metadata/._SUCCESS.crc  \n","  inflating: content/Model/stages/0_DocumentAssembler_63357943d98f/metadata/part-00000  \n"," extracting: content/Model/stages/0_DocumentAssembler_63357943d98f/metadata/_SUCCESS  \n","   creating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/\n","  inflating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/.bert_classification_tensorflow.crc  \n","   creating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/\n","   creating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/vocabulary/\n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/vocabulary/.part-00000.crc  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/vocabulary/._SUCCESS.crc  \n","  inflating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/vocabulary/part-00001  \n","  inflating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/vocabulary/part-00000  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/vocabulary/_SUCCESS  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/vocabulary/.part-00001.crc  \n","   creating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/signatures/\n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/signatures/.part-00000.crc  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/signatures/._SUCCESS.crc  \n","  inflating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/signatures/part-00001  \n","  inflating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/signatures/part-00000  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/signatures/_SUCCESS  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/signatures/.part-00001.crc  \n","   creating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/labels/\n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/labels/.part-00000.crc  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/labels/._SUCCESS.crc  \n","  inflating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/labels/part-00001  \n","  inflating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/labels/part-00000  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/labels/_SUCCESS  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/fields/labels/.part-00001.crc  \n","   creating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/metadata/\n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/metadata/.part-00000.crc  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/metadata/._SUCCESS.crc  \n","  inflating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/metadata/part-00000  \n"," extracting: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/metadata/_SUCCESS  \n","  inflating: content/Model/stages/2_BERT_FOR_SEQUENCE_CLASSIFICATION_41f87e548530/bert_classification_tensorflow  \n","   creating: content/Model/metadata/\n"," extracting: content/Model/metadata/.part-00000.crc  \n"," extracting: content/Model/metadata/._SUCCESS.crc  \n","  inflating: content/Model/metadata/part-00000  \n"," extracting: content/Model/metadata/_SUCCESS  \n"]}],"source":["!unzip \"/content/drive/MyDrive/Big Data Model/model.zip\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue91I1l61Jkx","executionInfo":{"status":"ok","timestamp":1690112219327,"user_tz":-420,"elapsed":212867,"user":{"displayName":"Lê Hoà","userId":"06201887992311635021"}},"outputId":"2c673a2a-eda8-4bbe-8302-afa4d89ef64c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dữ liệu 'ratings' rỗng hoặc bằng None. Dừng lại và không lưu vào file CSV.\n","                                          short_review    result\n","0    Value For Money yes Performance press hold to ...  positive\n","1    Performance good Value For Money yes Best Feat...  positive\n","2    Best Feature s very good Value For Money excel...  positive\n","3    Super fast delivery Received item one week aft...  positive\n","4    I like it fast delivery in good condition easy...  positive\n","..                                                 ...       ...\n","597                                            Love it  positive\n","598                                Thanks u the seller  positive\n","599                                               good  positive\n","600        Item received and it works thank you seller  positive\n","601                                              ok ok  positive\n","\n","[602 rows x 2 columns]\n","Number of positive reviews: 377\n","Number of negative reviews: 225\n"]}],"source":["from pyspark.sql import SparkSession, DataFrame\n","from pyspark.sql.functions import col, regexp_replace, trim\n","import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from pyspark.ml import Pipeline\n","from pyspark.ml import PipelineModel\n","from sklearn.metrics import classification_report\n","import re\n","import json\n","import requests\n","import pandas as pd\n","\n","url = \"https://shopee.sg/3-Month-Warranty-116-Plus-Smart-Watch-Blood-Pressure-Heart-Rate-Monitor-Waterproof-Fitness-Tracker-Watch-Smart-Band-i.139268812.6250395408?sp_atk=450a151\"\n","\n","r = re.search(r\"i\\.(\\d+)\\.(\\d+)\", url)\n","shop_id, item_id = r[1], r[2]\n","ratings_url = \"https://shopee.vn/api/v2/item/get_ratings?filter=0&flag=1&itemid={item_id}&limit=20&offset={offset}&shopid={shop_id}&type=0\"\n","\n","offset = 0\n","d = {\"username\": [], \"comment\": []}\n","flag_stop = False  # Biến cờ để kiểm tra điều kiện dừng vòng lặp\n","\n","while not flag_stop:\n","    data = requests.get(\n","        ratings_url.format(shop_id=shop_id, item_id=item_id, offset=offset)\n","    ).json()\n","\n","    if \"data\" not in data or \"ratings\" not in data[\"data\"]:\n","        print(\"Không có dữ liệu hoặc đã xảy ra lỗi.\")\n","        break\n","\n","    ratings = data[\"data\"][\"ratings\"]\n","    if not ratings:\n","        print(\"Dữ liệu 'ratings' rỗng hoặc bằng None. Dừng lại và không lưu vào file CSV.\")\n","        break\n","\n","    for rating in ratings:\n","        comment_text = rating[\"comment\"]\n","\n","        if comment_text is not None:  # Kiểm tra xem trường \"comment\" có giá trị hợp lệ hay không (không là None)\n","            comment_text = comment_text.strip()\n","\n","        d[\"username\"].append(rating[\"author_username\"])\n","        d[\"comment\"].append(comment_text)\n","\n","        # print(rating[\"author_username\"])\n","        # print(comment_text)\n","        # print(\"-\" * 100)\n","\n","    offset += 20\n","\n","# Lưu vào file CSV chỉ khi có dữ liệu có trường \"comment\" không rỗng\n","if d[\"comment\"]:\n","    df = pd.DataFrame(d)\n","    df.to_csv(\"dataEN.csv\", index=False)\n","else:\n","    print(\"Không có dữ liệu có trường 'comment' không rỗng để lưu vào file CSV.\")\n","\n","# Đọc dữ liệu từ file CSV vào DataFrame với tùy chọn multiLine=True để xử lý dữ liệu có nhiều dòng\n","df = spark.read.csv(\"dataEN.csv\", header=True, multiLine=True, inferSchema=True)\n","\n","# Đổi tên cột \"comment\" thành \"short_review\"\n","df = df.withColumnRenamed(\"comment\", \"short_review\")\n","\n","# Loại bỏ các dòng có trường \"short_review\" hoặc \"username\" rỗng từ DataFrame\n","df = df.dropna(subset=[\"short_review\", \"username\"])\n","df = df.dropna(subset=[\"short_review\"])\n","# Xoá các icon (ký tự không phải tiếng Anh) trong trường \"short_review\"\n","df = df.withColumn(\"short_review\", regexp_replace(\"short_review\", r\"[^a-zA-Z\\s]\", \" \"))\n","\n","# Thay thế các chuỗi gồm nhiều khoảng trắng liên tiếp bằng một khoảng trắng duy nhất\n","df = df.withColumn(\"short_review\", regexp_replace(\"short_review\", r\"\\s+\", \" \"))\n","\n","# Loại bỏ khoảng trắng ở đầu và cuối mỗi giá trị trong trường \"short_review\"\n","df = df.withColumn(\"short_review\", trim(col(\"short_review\")))\n","\n","# Lọc các dòng có trường \"short_review\" không rỗng\n","df = df.filter(col(\"short_review\") != \"\")\n","\n","\n","# Load model and motional analysis  --------------------------------------------------------------------\n","model_path = \"/content/content/Model\"\n","loaded_model = PipelineModel.load(model_path)\n","# Use the loaded model to make predictions on the new data\n","predictions = loaded_model.transform(df)\n","df = predictions.select(['short_review','label.result']).toPandas()\n","df['result'] = df['result'].apply(lambda x: x[0])\n","\n","# Process name label\n","# Function to map 'pos' to 'positive' and 'neg' to 'negative'\n","def map_sentiment(sentiment):\n","    if sentiment == 'pos':\n","        return 'positive'\n","    elif sentiment == 'neg':\n","        return 'negative'\n","    else:\n","        return sentiment\n","\n","# Applying the mapping function to the 'result' column\n","df['result'] = df['result'].apply(map_sentiment)\n","\n","# Displaying the DataFrame\n","print(df)\n","\n","positive_count = df[df['result'] == 'positive'].shape[0]\n","negative_count = df[df['result'] == 'negative'].shape[0]\n","print(f\"Number of positive reviews: {positive_count}\")\n","print(f\"Number of negative reviews: {negative_count}\")\n","\n","\n","# Saving the Pandas DataFrame to a CSV file\n","df.to_csv('predictions.csv', index=False)"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"15zBaf_sXc1VZhybU6c6dfqQ3y-9cciwY","authorship_tag":"ABX9TyPhY/UrJe9iLZ35xMEawxKk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}